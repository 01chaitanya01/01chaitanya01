{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZ9O8CmU7eJkxNJ7y3rsSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01chaitanya01/01chaitanya01/blob/main/Untitled36.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eAiwa3gLk7p9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from CSV\n",
        "data = pd.read_csv('/content/Gita-data.csv')\n",
        "\n",
        "# Tokenize Sanskrit and English texts\n",
        "sanskrit_tokenizer = Tokenizer()\n",
        "sanskrit_tokenizer.fit_on_texts(data['SA'])\n",
        "sanskrit_vocab_size = len(sanskrit_tokenizer.word_index) + 1\n",
        "\n",
        "english_tokenizer = Tokenizer()\n",
        "english_tokenizer.fit_on_texts(data['EN'])\n",
        "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences\n",
        "sanskrit_sequences = sanskrit_tokenizer.texts_to_sequences(data['SA'])\n",
        "english_sequences = english_tokenizer.texts_to_sequences(data['EN'])\n",
        "\n",
        "# Pad sequences for consistent input length\n",
        "max_sequence_length = max(max(len(seq) for seq in sanskrit_sequences), max(len(seq) for seq in english_sequences))\n",
        "sanskrit_sequences = pad_sequences(sanskrit_sequences, maxlen=max_sequence_length, padding='post')\n",
        "english_sequences = pad_sequences(english_sequences, maxlen=max_sequence_length, padding='post')\n"
      ],
      "metadata": {
        "id": "XyZ52r09lFA8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "input_sanskrit = Input(shape=(max_sequence_length,))\n",
        "input_english = Input(shape=(max_sequence_length,))\n",
        "\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "sanskrit_embedding = Embedding(sanskrit_vocab_size, embedding_dim)(input_sanskrit)\n",
        "english_embedding = Embedding(english_vocab_size, embedding_dim)(input_english)\n",
        "\n",
        "lstm = LSTM(lstm_units, return_sequences=True)(sanskrit_embedding)\n",
        "output_lstm = LSTM(lstm_units, return_sequences=True)(english_embedding)\n",
        "output = Dense(english_vocab_size, activation='softmax')(output_lstm)\n",
        "\n",
        "model = Model(inputs=[input_sanskrit, input_english], outputs=output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
      ],
      "metadata": {
        "id": "Gtwl6GQslUvt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "\n",
        "model.fit([sanskrit_sequences, english_sequences], np.expand_dims(english_sequences, -1), epochs=epochs, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuMBfJ70lYxW",
        "outputId": "5b9d0776-4f09-4446-af62-7cf675227447"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 60s 5s/step - loss: 4.4902\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 2.3460\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 2.3719\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 2.3500\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 2.1052\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 2.0437\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 55s 5s/step - loss: 1.9839\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.8856\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.7649\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.6709\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.6230\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.6009\n",
            "Epoch 13/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.5852\n",
            "Epoch 14/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 1.5720\n",
            "Epoch 15/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.5564\n",
            "Epoch 16/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.5406\n",
            "Epoch 17/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.5224\n",
            "Epoch 18/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.5028\n",
            "Epoch 19/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.4809\n",
            "Epoch 20/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.4569\n",
            "Epoch 21/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.4308\n",
            "Epoch 22/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.4045\n",
            "Epoch 23/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.3786\n",
            "Epoch 24/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.3532\n",
            "Epoch 25/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.3286\n",
            "Epoch 26/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.3043\n",
            "Epoch 27/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 1.2795\n",
            "Epoch 28/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.2537\n",
            "Epoch 29/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 1.2249\n",
            "Epoch 30/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 1.1930\n",
            "Epoch 31/50\n",
            "11/11 [==============================] - 60s 5s/step - loss: 1.1584\n",
            "Epoch 32/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.1226\n",
            "Epoch 33/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.0857\n",
            "Epoch 34/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 1.0482\n",
            "Epoch 35/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 1.0111\n",
            "Epoch 36/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.9741\n",
            "Epoch 37/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 0.9366\n",
            "Epoch 38/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.8998\n",
            "Epoch 39/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.8633\n",
            "Epoch 40/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.8276\n",
            "Epoch 41/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 0.7941\n",
            "Epoch 42/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.7602\n",
            "Epoch 43/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.7284\n",
            "Epoch 44/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.6983\n",
            "Epoch 45/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.6680\n",
            "Epoch 46/50\n",
            "11/11 [==============================] - 58s 5s/step - loss: 0.6393\n",
            "Epoch 47/50\n",
            "11/11 [==============================] - 56s 5s/step - loss: 0.6113\n",
            "Epoch 48/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 0.5846\n",
            "Epoch 49/50\n",
            "11/11 [==============================] - 57s 5s/step - loss: 0.5586\n",
            "Epoch 50/50\n",
            "11/11 [==============================] - 59s 5s/step - loss: 0.5339\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fcab10a2b00>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence):\n",
        "    sequence = sanskrit_tokenizer.texts_to_sequences([sentence])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
        "    translation = model.predict([padded_sequence, padded_sequence])\n",
        "    translation = np.argmax(translation, axis=-1)\n",
        "    translated_sentence = ' '.join(english_tokenizer.index_word[i] for i in translation[0] if i != 0)\n",
        "    return translated_sentence\n"
      ],
      "metadata": {
        "id": "z1kf6uhIlgI7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sanskrit_sentence = \"सञ्जय उवाच ।दृष्ट्वा तु पाण्डवानीकं व्यूढं दुर्योधनस्तदा ।आचार्यमुपसङ्गम्य राजा वचनमब्रवीत्\"\n",
        "translated_sentence = translate_sentence(sanskrit_sentence)\n",
        "print(\"Translated sentence:\", translated_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj4ZnGXlz5dr",
        "outputId": "aeb35c77-1a21-46a9-9a01-75f4bf05c0ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 822ms/step\n",
            "Translated sentence: sanjay who attached by goal veda veda existence attain battle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0SJdG_Kwz74B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}